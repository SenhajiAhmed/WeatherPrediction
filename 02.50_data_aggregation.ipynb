{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99fc489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports loaded\n",
      "Configuration: Window size = 7 days\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "INPUT_FILE = 'era5_daily_features.csv'\n",
    "WINDOW_SIZE = 7  # Days to look back\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Imports loaded\")\n",
    "print(f\"Configuration: Window size = {WINDOW_SIZE} days\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f2e8297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 70,200 rows\n",
      "Date range: 2025-01-01 00:00:00 to 2025-01-30 00:00:00\n",
      "\n",
      "Columns (27): ['date', 'latitude', 'longitude', 't2m_min', 't2m_max', 't2m_mean', 'd2m_min', 'd2m_max', 'd2m_mean', 'msl_min', 'msl_max', 'msl_mean', 'u10_min', 'u10_max', 'u10_mean', 'v10_min', 'v10_max', 'v10_mean', 'tcc_min', 'tcc_max', 'tcc_mean', 'skt_min', 'skt_max', 'skt_mean', 't2m_min_next', 't2m_max_next', 't2m_mean_next']\n",
      "\n",
      "Number of locations: 2340\n",
      "\n",
      "✓ Target columns found: ['t2m_min_next', 't2m_max_next', 't2m_mean_next']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>t2m_min</th>\n",
       "      <th>t2m_max</th>\n",
       "      <th>t2m_mean</th>\n",
       "      <th>d2m_min</th>\n",
       "      <th>d2m_max</th>\n",
       "      <th>d2m_mean</th>\n",
       "      <th>msl_min</th>\n",
       "      <th>...</th>\n",
       "      <th>v10_mean</th>\n",
       "      <th>tcc_min</th>\n",
       "      <th>tcc_max</th>\n",
       "      <th>tcc_mean</th>\n",
       "      <th>skt_min</th>\n",
       "      <th>skt_max</th>\n",
       "      <th>skt_mean</th>\n",
       "      <th>t2m_min_next</th>\n",
       "      <th>t2m_max_next</th>\n",
       "      <th>t2m_mean_next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-9.8</td>\n",
       "      <td>8.079742</td>\n",
       "      <td>19.427155</td>\n",
       "      <td>13.512838</td>\n",
       "      <td>-2.99888</td>\n",
       "      <td>-0.61387</td>\n",
       "      <td>-2.062680</td>\n",
       "      <td>1017.82560</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.016637</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.984451</td>\n",
       "      <td>0.293353</td>\n",
       "      <td>4.30374</td>\n",
       "      <td>26.47450</td>\n",
       "      <td>13.539474</td>\n",
       "      <td>8.284332</td>\n",
       "      <td>20.229400</td>\n",
       "      <td>13.733927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-02</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-9.8</td>\n",
       "      <td>8.284332</td>\n",
       "      <td>20.229400</td>\n",
       "      <td>13.733927</td>\n",
       "      <td>-7.56992</td>\n",
       "      <td>-2.55112</td>\n",
       "      <td>-4.270289</td>\n",
       "      <td>1018.21970</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.549742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.495850</td>\n",
       "      <td>0.053050</td>\n",
       "      <td>4.47048</td>\n",
       "      <td>25.16592</td>\n",
       "      <td>13.271984</td>\n",
       "      <td>8.742096</td>\n",
       "      <td>20.117584</td>\n",
       "      <td>13.730153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-03</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-9.8</td>\n",
       "      <td>8.742096</td>\n",
       "      <td>20.117584</td>\n",
       "      <td>13.730153</td>\n",
       "      <td>-7.83945</td>\n",
       "      <td>0.59340</td>\n",
       "      <td>-2.821040</td>\n",
       "      <td>1017.91875</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.010354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424484</td>\n",
       "      <td>0.053706</td>\n",
       "      <td>5.53707</td>\n",
       "      <td>25.11807</td>\n",
       "      <td>13.454448</td>\n",
       "      <td>8.493073</td>\n",
       "      <td>19.262848</td>\n",
       "      <td>13.515086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-04</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-9.8</td>\n",
       "      <td>8.493073</td>\n",
       "      <td>19.262848</td>\n",
       "      <td>13.515086</td>\n",
       "      <td>-4.86875</td>\n",
       "      <td>-0.84434</td>\n",
       "      <td>-2.462602</td>\n",
       "      <td>1017.33800</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.404602</td>\n",
       "      <td>0.545135</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.881154</td>\n",
       "      <td>5.31387</td>\n",
       "      <td>25.00650</td>\n",
       "      <td>13.461613</td>\n",
       "      <td>9.574616</td>\n",
       "      <td>21.778473</td>\n",
       "      <td>14.880564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-05</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-9.8</td>\n",
       "      <td>9.574616</td>\n",
       "      <td>21.778473</td>\n",
       "      <td>14.880564</td>\n",
       "      <td>-3.36777</td>\n",
       "      <td>0.49940</td>\n",
       "      <td>-1.195697</td>\n",
       "      <td>1014.91860</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.352264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.470805</td>\n",
       "      <td>5.87686</td>\n",
       "      <td>29.31753</td>\n",
       "      <td>15.110692</td>\n",
       "      <td>8.145905</td>\n",
       "      <td>22.907135</td>\n",
       "      <td>14.977478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  latitude  longitude   t2m_min    t2m_max   t2m_mean  d2m_min  \\\n",
       "0 2025-01-01      20.0       -9.8  8.079742  19.427155  13.512838 -2.99888   \n",
       "1 2025-01-02      20.0       -9.8  8.284332  20.229400  13.733927 -7.56992   \n",
       "2 2025-01-03      20.0       -9.8  8.742096  20.117584  13.730153 -7.83945   \n",
       "3 2025-01-04      20.0       -9.8  8.493073  19.262848  13.515086 -4.86875   \n",
       "4 2025-01-05      20.0       -9.8  9.574616  21.778473  14.880564 -3.36777   \n",
       "\n",
       "   d2m_max  d2m_mean     msl_min  ...  v10_mean   tcc_min   tcc_max  tcc_mean  \\\n",
       "0 -0.61387 -2.062680  1017.82560  ... -3.016637  0.000000  0.984451  0.293353   \n",
       "1 -2.55112 -4.270289  1018.21970  ... -2.549742  0.000000  0.495850  0.053050   \n",
       "2  0.59340 -2.821040  1017.91875  ... -2.010354  0.000000  0.424484  0.053706   \n",
       "3 -0.84434 -2.462602  1017.33800  ... -1.404602  0.545135  1.000000  0.881154   \n",
       "4  0.49940 -1.195697  1014.91860  ... -2.352264  0.000000  1.000000  0.470805   \n",
       "\n",
       "   skt_min   skt_max   skt_mean  t2m_min_next  t2m_max_next  t2m_mean_next  \n",
       "0  4.30374  26.47450  13.539474      8.284332     20.229400      13.733927  \n",
       "1  4.47048  25.16592  13.271984      8.742096     20.117584      13.730153  \n",
       "2  5.53707  25.11807  13.454448      8.493073     19.262848      13.515086  \n",
       "3  5.31387  25.00650  13.461613      9.574616     21.778473      14.880564  \n",
       "4  5.87686  29.31753  15.110692      8.145905     22.907135      14.977478  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(INPUT_FILE, parse_dates=['date'])\n",
    "print(f\"Loaded {len(df):,} rows\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nColumns ({len(df.columns)}): {list(df.columns)}\")\n",
    "print(f\"\\nNumber of locations: {df.groupby(['latitude', 'longitude']).ngroups}\")\n",
    "\n",
    "# Check for target columns\n",
    "target_cols = ['t2m_min_next', 't2m_max_next', 't2m_mean_next']\n",
    "has_targets = all(col in df.columns for col in target_cols)\n",
    "\n",
    "if has_targets:\n",
    "    print(\"\\n✓ Target columns found:\", target_cols)\n",
    "else:\n",
    "    print(\"\\n⚠️  WARNING: Missing target columns!\")\n",
    "    print(\"Expected:\", target_cols)\n",
    "\n",
    "# Display sample\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c34dd416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Feature Columns (21) ===\n",
      " 1. t2m_min\n",
      " 2. t2m_max\n",
      " 3. t2m_mean\n",
      " 4. d2m_min\n",
      " 5. d2m_max\n",
      " 6. d2m_mean\n",
      " 7. msl_min\n",
      " 8. msl_max\n",
      " 9. msl_mean\n",
      "10. u10_min\n",
      "11. u10_max\n",
      "12. u10_mean\n",
      "13. v10_min\n",
      "14. v10_max\n",
      "15. v10_mean\n",
      "16. tcc_min\n",
      "17. tcc_max\n",
      "18. tcc_mean\n",
      "19. skt_min\n",
      "20. skt_max\n",
      "21. skt_mean\n",
      "\n",
      "=== Target Columns (3) ===\n",
      "1. t2m_min_next\n",
      "2. t2m_max_next\n",
      "3. t2m_mean_next\n"
     ]
    }
   ],
   "source": [
    "# Exclude targets and identifiers from features\n",
    "exclude_cols = target_cols + ['date', 'latitude', 'longitude']\n",
    "feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "\n",
    "print(f\"=== Feature Columns ({len(feature_cols)}) ===\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\n=== Target Columns ({len(target_cols)}) ===\")\n",
    "for i, col in enumerate(target_cols, 1):\n",
    "    print(f\"{i}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0129e18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "REGION & DATA COVERAGE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "--- Region Statistics ---\n",
      "Total unique regions (lat/lon combinations): 2340\n",
      "Average rows per region: 30.0\n",
      "Min rows per region: 30\n",
      "Max rows per region: 30\n",
      "Median rows per region: 30.0\n",
      "\n",
      "--- Region Distribution ---\n",
      "      latitude  longitude  row_count\n",
      "2339      36.0      -1.05         30\n",
      "0         20.0      -9.80         30\n",
      "1         20.0      -9.55         30\n",
      "2         20.0      -9.30         30\n",
      "2323      36.0      -5.05         30\n",
      "2322      36.0      -5.30         30\n",
      "2321      36.0      -5.55         30\n",
      "2320      36.0      -5.80         30\n",
      "2319      36.0      -6.05         30\n",
      "2318      36.0      -6.30         30\n",
      "\n",
      "--- Temporal Coverage Per Region ---\n",
      "   latitude  longitude first_date  last_date  n_days  duration_days  \\\n",
      "0      20.0      -9.80 2025-01-01 2025-01-30      30             30   \n",
      "1      20.0      -9.55 2025-01-01 2025-01-30      30             30   \n",
      "2      20.0      -9.30 2025-01-01 2025-01-30      30             30   \n",
      "3      20.0      -9.05 2025-01-01 2025-01-30      30             30   \n",
      "4      20.0      -8.80 2025-01-01 2025-01-30      30             30   \n",
      "5      20.0      -8.55 2025-01-01 2025-01-30      30             30   \n",
      "6      20.0      -8.30 2025-01-01 2025-01-30      30             30   \n",
      "7      20.0      -8.05 2025-01-01 2025-01-30      30             30   \n",
      "8      20.0      -7.80 2025-01-01 2025-01-30      30             30   \n",
      "9      20.0      -7.55 2025-01-01 2025-01-30      30             30   \n",
      "\n",
      "   coverage_pct  \n",
      "0         100.0  \n",
      "1         100.0  \n",
      "2         100.0  \n",
      "3         100.0  \n",
      "4         100.0  \n",
      "5         100.0  \n",
      "6         100.0  \n",
      "7         100.0  \n",
      "8         100.0  \n",
      "9         100.0  \n",
      "\n",
      "Overall temporal statistics:\n",
      "  Mean coverage: 100.0%\n",
      "  Min days per region: 30\n",
      "  Max days per region: 30\n",
      "  Total duration range: 30 - 30 days\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# COMPREHENSIVE REGION & DATA ANALYSIS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"REGION & DATA COVERAGE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. COUNT UNIQUE REGIONS\n",
    "regions = df.groupby(['latitude', 'longitude']).size().reset_index(name='row_count')\n",
    "n_regions = len(regions)\n",
    "\n",
    "print(f\"\\n--- Region Statistics ---\")\n",
    "print(f\"Total unique regions (lat/lon combinations): {n_regions}\")\n",
    "print(f\"Average rows per region: {regions['row_count'].mean():.1f}\")\n",
    "print(f\"Min rows per region: {regions['row_count'].min()}\")\n",
    "print(f\"Max rows per region: {regions['row_count'].max()}\")\n",
    "print(f\"Median rows per region: {regions['row_count'].median():.1f}\")\n",
    "\n",
    "# Show region distribution\n",
    "print(f\"\\n--- Region Distribution ---\")\n",
    "print(regions.sort_values('row_count', ascending=False).head(10))\n",
    "\n",
    "# 2. TEMPORAL COVERAGE PER REGION\n",
    "print(f\"\\n--- Temporal Coverage Per Region ---\")\n",
    "temporal_stats = df.groupby(['latitude', 'longitude']).agg({\n",
    "    'date': ['min', 'max', 'count']\n",
    "}).reset_index()\n",
    "temporal_stats.columns = ['latitude', 'longitude', 'first_date', 'last_date', 'n_days']\n",
    "temporal_stats['duration_days'] = (pd.to_datetime(temporal_stats['last_date']) - \n",
    "                                    pd.to_datetime(temporal_stats['first_date'])).dt.days + 1\n",
    "temporal_stats['coverage_pct'] = (temporal_stats['n_days'] / temporal_stats['duration_days'] * 100)\n",
    "\n",
    "print(temporal_stats.head(10))\n",
    "\n",
    "print(f\"\\nOverall temporal statistics:\")\n",
    "print(f\"  Mean coverage: {temporal_stats['coverage_pct'].mean():.1f}%\")\n",
    "print(f\"  Min days per region: {temporal_stats['n_days'].min()}\")\n",
    "print(f\"  Max days per region: {temporal_stats['n_days'].max()}\")\n",
    "print(f\"  Total duration range: {temporal_stats['duration_days'].min()} - {temporal_stats['duration_days'].max()} days\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0fc07c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATA LEAKAGE RISK ASSESSMENT\n",
      "======================================================================\n",
      "\n",
      "--- Sample Generation Analysis ---\n",
      "Theoretical max samples (rows - window_size): 53,820\n",
      "Actual samples created: 53,820\n",
      "Loss due to targets/NaN: 0 (0.00%)\n",
      "\n",
      "--- Feature-Target Temporal Relationship ---\n",
      "Samples checked: 100\n",
      "Temporal leaks found: 0 (0.00%)\n",
      "✓ No temporal leakage detected\n",
      "\n",
      "--- Target Independence Analysis ---\n",
      "\n",
      "t2m_min_next:\n",
      "  Correlation with last_value_t-1      : 0.8038  ✓ Normal\n",
      "  Correlation with window_mean         : 0.7319  ✓ Normal\n",
      "  Correlation with trend               : 0.3570  ✓ Normal\n",
      "\n",
      "t2m_max_next:\n",
      "  Correlation with last_value_t-1      : 0.7723  ✓ Normal\n",
      "  Correlation with window_mean         : 0.5810  ✓ Normal\n",
      "  Correlation with trend               : 0.5145  ✓ Normal\n",
      "\n",
      "t2m_mean_next:\n",
      "  Correlation with last_value_t-1      : 0.8232  ✓ Normal\n",
      "  Correlation with window_mean         : 0.6659  ✓ Normal\n",
      "  Correlation with trend               : 0.4899  ✓ Normal\n",
      "\n",
      "--- Value Matching Analysis (Leakage Detection) ---\n",
      "\n",
      "t2m_min_next vs t2m_min_last:\n",
      "  Exact matches (0K diff):          3 / 53820 ( 0.01%)\n",
      "  Very close (< 0.01K diff):      165 / 53820 ( 0.31%)\n",
      "  Reasonable (< 1K diff):       15654 / 53820 (29.09%)\n",
      "  ✓ Good: Target shows independence from recent values\n",
      "\n",
      "t2m_max_next vs t2m_max_last:\n",
      "  Exact matches (0K diff):          3 / 53820 ( 0.01%)\n",
      "  Very close (< 0.01K diff):      167 / 53820 ( 0.31%)\n",
      "  Reasonable (< 1K diff):       13874 / 53820 (25.78%)\n",
      "  ✓ Good: Target shows independence from recent values\n",
      "\n",
      "t2m_mean_next vs t2m_mean_last:\n",
      "  Exact matches (0K diff):          0 / 53820 ( 0.00%)\n",
      "  Very close (< 0.01K diff):      156 / 53820 ( 0.29%)\n",
      "  Reasonable (< 1K diff):       16941 / 53820 (31.48%)\n",
      "  ✓ Good: Target shows independence from recent values\n",
      "\n",
      "--- Cross-Validation Leakage Risk ---\n",
      "\n",
      "Regions with overlapping time periods:\n",
      "  Global date range: 2025-01-01 00:00:00 to 2025-01-30 00:00:00\n",
      "  Total span: 29 days\n",
      "  Regions covering full span: 2340 / 2340\n",
      "  ⚠️  All regions have same temporal coverage\n",
      "  → Risk: Regional leakage if not using location-aware splits\n",
      "\n",
      "======================================================================\n",
      "LEAKAGE ASSESSMENT SUMMARY\n",
      "======================================================================\n",
      "✓ Temporal integrity: PASS (no future data in features)\n",
      "✓ Correlation check: PASS (max correlation = 0.8232)\n",
      "✓ Value matching: PASS (avg exact match = 0.00%)\n",
      "✓ Sample integrity: PASS (0.0% loss is acceptable)\n",
      "✓ Data quality: PASS (no NaN values)\n",
      "\n",
      "======================================================================\n",
      "OVERALL LEAKAGE SCORE: 5/5\n",
      "======================================================================\n",
      "🎉 EXCELLENT: No data leakage detected. Safe to train!\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DATA LEAKAGE PREDICTION & ANALYSIS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA LEAKAGE RISK ASSESSMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 3. THEORETICAL vs ACTUAL SAMPLES\n",
    "print(\"\\n--- Sample Generation Analysis ---\")\n",
    "theoretical_samples = sum([max(0, count - WINDOW_SIZE) for count in regions['row_count']])\n",
    "actual_samples = len(X_agg)\n",
    "\n",
    "print(f\"Theoretical max samples (rows - window_size): {theoretical_samples:,}\")\n",
    "print(f\"Actual samples created: {actual_samples:,}\")\n",
    "print(f\"Loss due to targets/NaN: {theoretical_samples - actual_samples:,} ({(1 - actual_samples/theoretical_samples)*100:.2f}%)\")\n",
    "\n",
    "# 4. FEATURE-TARGET TEMPORAL RELATIONSHIP\n",
    "print(\"\\n--- Feature-Target Temporal Relationship ---\")\n",
    "\n",
    "# Randomly sample to check temporal integrity\n",
    "sample_indices = np.random.choice(len(X_agg), min(100, len(X_agg)), replace=False)\n",
    "\n",
    "temporal_leaks = 0\n",
    "for idx in sample_indices:\n",
    "    pred_date = X_agg['date'].iloc[idx]\n",
    "    lat, lon = X_agg['latitude'].iloc[idx], X_agg['longitude'].iloc[idx]\n",
    "    \n",
    "    # Get original data for this region\n",
    "    region_data = df[(df['latitude'] == lat) & (df['longitude'] == lon)].sort_values('date')\n",
    "    \n",
    "    # Find the prediction date in original data\n",
    "    pred_row_idx = region_data[region_data['date'] == pred_date].index\n",
    "    \n",
    "    if len(pred_row_idx) > 0:\n",
    "        pred_row_idx = region_data.index.get_loc(pred_row_idx[0])\n",
    "        \n",
    "        # Check if any feature uses data from prediction date or after\n",
    "        if pred_row_idx < WINDOW_SIZE:\n",
    "            temporal_leaks += 1\n",
    "\n",
    "leak_pct = (temporal_leaks / len(sample_indices)) * 100\n",
    "print(f\"Samples checked: {len(sample_indices)}\")\n",
    "print(f\"Temporal leaks found: {temporal_leaks} ({leak_pct:.2f}%)\")\n",
    "\n",
    "if leak_pct > 0:\n",
    "    print(\"⚠️  WARNING: Features using data from prediction date or future!\")\n",
    "else:\n",
    "    print(\"✓ No temporal leakage detected\")\n",
    "\n",
    "\n",
    "# 5. TARGET INDEPENDENCE ANALYSIS\n",
    "print(\"\\n--- Target Independence Analysis ---\")\n",
    "\n",
    "independence_results = {}\n",
    "\n",
    "for target_col in target_cols:\n",
    "    base_col = target_col.replace('_next', '')\n",
    "    \n",
    "    # Check correlation with different time lags\n",
    "    correlations = {}\n",
    "    \n",
    "    if f\"{base_col}_last\" in X_agg.columns:\n",
    "        # Last value (t-1)\n",
    "        corr_last = X_agg[f\"{base_col}_last\"].corr(y_agg[target_col])\n",
    "        correlations['last_value_t-1'] = corr_last\n",
    "        \n",
    "    if f\"{base_col}_mean\" in X_agg.columns:\n",
    "        # Window mean\n",
    "        corr_mean = X_agg[f\"{base_col}_mean\"].corr(y_agg[target_col])\n",
    "        correlations['window_mean'] = corr_mean\n",
    "    \n",
    "    if f\"{base_col}_trend\" in X_agg.columns:\n",
    "        # Trend\n",
    "        corr_trend = X_agg[f\"{base_col}_trend\"].corr(y_agg[target_col])\n",
    "        correlations['trend'] = corr_trend\n",
    "    \n",
    "    independence_results[target_col] = correlations\n",
    "    \n",
    "    print(f\"\\n{target_col}:\")\n",
    "    for feat_name, corr in correlations.items():\n",
    "        status = \"⚠️  HIGH\" if abs(corr) > 0.95 else \"✓ Normal\"\n",
    "        print(f\"  Correlation with {feat_name:20s}: {corr:6.4f}  {status}\")\n",
    "        \n",
    "        if abs(corr) > 0.95:\n",
    "            print(f\"    → Potential leakage: target too similar to feature\")\n",
    "\n",
    "\n",
    "# 6. VALUE MATCHING ANALYSIS\n",
    "print(\"\\n--- Value Matching Analysis (Leakage Detection) ---\")\n",
    "\n",
    "for target_col in target_cols:\n",
    "    base_col = target_col.replace('_next', '')\n",
    "    \n",
    "    if f\"{base_col}_last\" in X_agg.columns:\n",
    "        last_feature = X_agg[f\"{base_col}_last\"].values\n",
    "        target_values = y_agg[target_col].values\n",
    "        \n",
    "        # Check exact matches (suspicious)\n",
    "        exact_matches = (last_feature == target_values).sum()\n",
    "        \n",
    "        # Check very close matches (< 0.01K difference)\n",
    "        close_matches = (np.abs(last_feature - target_values) < 0.01).sum()\n",
    "        \n",
    "        # Check reasonable matches (< 1K difference - expected for persistent weather)\n",
    "        reasonable_matches = (np.abs(last_feature - target_values) < 1.0).sum()\n",
    "        \n",
    "        total = len(target_values)\n",
    "        \n",
    "        print(f\"\\n{target_col} vs {base_col}_last:\")\n",
    "        print(f\"  Exact matches (0K diff):      {exact_matches:5d} / {total:5d} ({exact_matches/total*100:5.2f}%)\")\n",
    "        print(f\"  Very close (< 0.01K diff):    {close_matches:5d} / {total:5d} ({close_matches/total*100:5.2f}%)\")\n",
    "        print(f\"  Reasonable (< 1K diff):       {reasonable_matches:5d} / {total:5d} ({reasonable_matches/total*100:5.2f}%)\")\n",
    "        \n",
    "        # Interpretation\n",
    "        if exact_matches / total > 0.5:\n",
    "            print(f\"  🚨 CRITICAL: >50% exact matches - DEFINITE LEAKAGE!\")\n",
    "        elif exact_matches / total > 0.1:\n",
    "            print(f\"  ⚠️  WARNING: >10% exact matches - Possible leakage\")\n",
    "        elif close_matches / total > 0.5:\n",
    "            print(f\"  ⚠️  SUSPICIOUS: >50% very close matches\")\n",
    "        elif reasonable_matches / total > 0.7:\n",
    "            print(f\"  ✓ Normal: High persistence expected in weather data\")\n",
    "        else:\n",
    "            print(f\"  ✓ Good: Target shows independence from recent values\")\n",
    "\n",
    "\n",
    "# 7. CROSS-VALIDATION LEAKAGE CHECK\n",
    "print(\"\\n--- Cross-Validation Leakage Risk ---\")\n",
    "\n",
    "# Check if dates overlap between train/test in different regions\n",
    "date_ranges_per_region = df.groupby(['latitude', 'longitude'])['date'].agg(['min', 'max'])\n",
    "date_ranges_per_region['span'] = (pd.to_datetime(date_ranges_per_region['max']) - \n",
    "                                   pd.to_datetime(date_ranges_per_region['min'])).dt.days\n",
    "\n",
    "print(f\"\\nRegions with overlapping time periods:\")\n",
    "min_date = df['date'].min()\n",
    "max_date = df['date'].max()\n",
    "print(f\"  Global date range: {min_date} to {max_date}\")\n",
    "print(f\"  Total span: {(max_date - min_date).days} days\")\n",
    "\n",
    "# Check if all regions span the same time\n",
    "regions_full_span = (date_ranges_per_region['span'] == (max_date - min_date).days).sum()\n",
    "print(f\"  Regions covering full span: {regions_full_span} / {n_regions}\")\n",
    "\n",
    "if regions_full_span == n_regions:\n",
    "    print(\"  ⚠️  All regions have same temporal coverage\")\n",
    "    print(\"  → Risk: Regional leakage if not using location-aware splits\")\n",
    "else:\n",
    "    print(\"  ✓ Regions have different temporal coverage\")\n",
    "    print(\"  → Lower risk of spatial leakage\")\n",
    "\n",
    "\n",
    "# 8. FINAL SUMMARY\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LEAKAGE ASSESSMENT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "leakage_score = 0\n",
    "max_score = 5\n",
    "\n",
    "# Criterion 1: Temporal integrity\n",
    "if leak_pct == 0:\n",
    "    print(\"✓ Temporal integrity: PASS (no future data in features)\")\n",
    "    leakage_score += 1\n",
    "else:\n",
    "    print(f\"✗ Temporal integrity: FAIL ({leak_pct:.1f}% temporal leaks)\")\n",
    "\n",
    "# Criterion 2: Target independence (correlation)\n",
    "max_corr = max([max(corrs.values()) for corrs in independence_results.values()])\n",
    "if max_corr < 0.95:\n",
    "    print(f\"✓ Correlation check: PASS (max correlation = {max_corr:.4f})\")\n",
    "    leakage_score += 1\n",
    "else:\n",
    "    print(f\"✗ Correlation check: FAIL (max correlation = {max_corr:.4f})\")\n",
    "\n",
    "# Criterion 3: Exact value matches\n",
    "avg_exact_match = np.mean([\n",
    "    (X_agg[f\"{col.replace('_next', '')}_last\"] == y_agg[col]).sum() / len(y_agg)\n",
    "    for col in target_cols if f\"{col.replace('_next', '')}_last\" in X_agg.columns\n",
    "])\n",
    "if avg_exact_match < 0.1:\n",
    "    print(f\"✓ Value matching: PASS (avg exact match = {avg_exact_match*100:.2f}%)\")\n",
    "    leakage_score += 1\n",
    "else:\n",
    "    print(f\"✗ Value matching: FAIL (avg exact match = {avg_exact_match*100:.2f}%)\")\n",
    "\n",
    "# Criterion 4: Sample count integrity\n",
    "sample_loss_pct = (1 - actual_samples/theoretical_samples)*100\n",
    "if sample_loss_pct < 15:\n",
    "    print(f\"✓ Sample integrity: PASS ({sample_loss_pct:.1f}% loss is acceptable)\")\n",
    "    leakage_score += 1\n",
    "else:\n",
    "    print(f\"⚠️  Sample integrity: WARNING ({sample_loss_pct:.1f}% loss)\")\n",
    "\n",
    "# Criterion 5: No NaN contamination\n",
    "if X_agg[feature_only_cols].isnull().sum().sum() == 0 and y_agg.isnull().sum().sum() == 0:\n",
    "    print(\"✓ Data quality: PASS (no NaN values)\")\n",
    "    leakage_score += 1\n",
    "else:\n",
    "    print(\"✗ Data quality: FAIL (NaN values present)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"OVERALL LEAKAGE SCORE: {leakage_score}/{max_score}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if leakage_score == max_score:\n",
    "    print(\"🎉 EXCELLENT: No data leakage detected. Safe to train!\")\n",
    "elif leakage_score >= 3:\n",
    "    print(\"✓ GOOD: Minor issues but generally safe to proceed\")\n",
    "elif leakage_score >= 2:\n",
    "    print(\"⚠️  WARNING: Some leakage concerns. Review carefully before training\")\n",
    "else:\n",
    "    print(\"🚨 CRITICAL: Significant leakage detected. DO NOT TRAIN until fixed!\")\n",
    "\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "563d85de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved features_aggregated.csv (53820 rows, 171 columns)\n",
      "✓ Saved targets_aggregated.csv (53820 rows, 3 columns)\n",
      "✓ Saved data_combined_aggregated.csv (53820 rows, 174 columns)\n"
     ]
    }
   ],
   "source": [
    "# Save aggregated features and targets to CSV\n",
    "X_agg.to_csv('features_aggregated.csv', index=False)\n",
    "y_agg.to_csv('targets_aggregated.csv', index=False)\n",
    "\n",
    "print(f\"✓ Saved features_aggregated.csv ({X_agg.shape[0]} rows, {X_agg.shape[1]} columns)\")\n",
    "print(f\"✓ Saved targets_aggregated.csv ({y_agg.shape[0]} rows, {y_agg.shape[1]} columns)\")\n",
    "\n",
    "# Optional: Save combined (features + targets in one file)\n",
    "combined = pd.concat([X_agg, y_agg], axis=1)\n",
    "combined.to_csv('data_combined_aggregated.csv', index=False)\n",
    "print(f\"✓ Saved data_combined_aggregated.csv ({combined.shape[0]} rows, {combined.shape[1]} columns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7759523e",
   "metadata": {},
   "source": [
    "# 📊 Feature Engineering: Understanding Window-Based Features\n",
    "\n",
    "## 🎯 What Features Are Created?\n",
    "\n",
    "For a **7-day sliding window**, each original feature (e.g., `t2m_min`) is transformed into **8 statistical features**:\n",
    "\n",
    "| Feature Name | Description | Time Reference | Purpose |\n",
    "|--------------|-------------|----------------|---------|\n",
    "| `t2m_min_last` | Most recent value | Day t-1 (yesterday) | Captures immediate persistence |\n",
    "| `t2m_min_first` | Oldest value in window | Day t-7 (7 days ago) | Baseline reference point |\n",
    "| `t2m_min_mean` | Average over window | Days t-7 to t-1 | Overall trend level |\n",
    "| `t2m_min_std` | Standard deviation | Days t-7 to t-1 | Variability/stability |\n",
    "| `t2m_min_min` | Minimum in window | Days t-7 to t-1 | Extreme cold point |\n",
    "| `t2m_min_max` | Maximum in window | Days t-7 to t-1 | Extreme warm point |\n",
    "| `t2m_min_trend` | Change (last - first) | t-1 minus t-7 | Warming/cooling direction |\n",
    "| `t2m_min_recent_3d` | Recent average | Days t-3, t-2, t-1 | Short-term trend |\n",
    "\n",
    "---\n",
    "\n",
    "## 📅 Temporal Structure\n",
    "\n",
    "### Timeline Visualization:\n",
    "\n",
    "```\n",
    "Past ←─────────────────────────────── Present ───→ Future (Prediction)\n",
    "┌────┬────┬────┬────┬────┬────┬────┬────────────┬──────────┐\n",
    "│t-7 │t-6 │t-5 │t-4 │t-3 │t-2 │t-1 │   TODAY    │ TOMORROW │\n",
    "└────┴────┴────┴────┴────┴────┴────┴────────────┴──────────┘\n",
    "  ↑                        └────┬─────┘      ↑         ↑\n",
    "  │                             │            │         │\n",
    "_first                    _recent_3d      _last    TARGET\n",
    "                                                  (predict this)\n",
    "```\n",
    "\n",
    "### Feature Coverage:\n",
    "\n",
    "- **`_first`** = Day t-7 (start of window)\n",
    "- **`_recent_3d`** = Average of Days t-3, t-2, t-1\n",
    "- **`_last`** = Day t-1 (most recent observation)\n",
    "- **Target** = Day t (what we're predicting)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Lag Interpretation\n",
    "\n",
    "Our aggregated features already **implicitly contain lag information**:\n",
    "\n",
    "| Aggregated Feature | Equivalent Lag Concept |\n",
    "|--------------------|------------------------|\n",
    "| `_last` | Lag 1 (yesterday) |\n",
    "| `_recent_3d` | Average of Lags 1, 2, 3 |\n",
    "| `_first` | Lag 7 (week ago) |\n",
    "| `_mean` | Average of Lags 1-7 |\n",
    "| `_trend` | Lag 7 → Lag 1 change |\n",
    "\n",
    "### Why This Is Better Than Explicit Lags:\n",
    "\n",
    "**Option A: Aggregated Features (Current Approach)** ✅\n",
    "```python\n",
    "# 8 features per variable\n",
    "t2m_min_mean, t2m_min_std, t2m_min_min, t2m_min_max,\n",
    "t2m_min_last, t2m_min_first, t2m_min_trend, t2m_min_recent_3d\n",
    "```\n",
    "\n",
    "**Option B: Explicit Lags** ❌ (Not recommended for Random Forest)\n",
    "```python\n",
    "# 7 features per variable\n",
    "t2m_min_lag1, t2m_min_lag2, t2m_min_lag3, t2m_min_lag4,\n",
    "t2m_min_lag5, t2m_min_lag6, t2m_min_lag7\n",
    "```\n",
    "\n",
    "### Advantages of Aggregated Features:\n",
    "\n",
    "| Aspect | Aggregated | Explicit Lags |\n",
    "|--------|-----------|---------------|\n",
    "| **Captures trends** | ✅ Yes (_trend, _mean) | ❌ No |\n",
    "| **Captures variability** | ✅ Yes (_std) | ❌ No |\n",
    "| **Handles noise** | ✅ Better (averaging) | ❌ Sensitive |\n",
    "| **Feature count** | ✅ Compact (8 per var) | ❌ Grows with lags |\n",
    "| **Works with RF** | ✅ Excellent | ⚠️ Okay |\n",
    "| **Works with LSTM** | ⚠️ Okay | ✅ Excellent |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Data Leakage Considerations\n",
    "\n",
    "### ✅ Safe: Time-Based Splits\n",
    "\n",
    "```python\n",
    "# Correct approach (what we use)\n",
    "Train: [════════════════════════] 80% (oldest data)\n",
    "Test:                           [═══] 20% (newest data)\n",
    "       \n",
    "Timeline: 2020 ──────────────→ 2023 ────→ 2024\n",
    "```\n",
    "\n",
    "**No leakage** because:\n",
    "- Training data is from the past\n",
    "- Test data is from the future\n",
    "- No information flows backward in time\n",
    "\n",
    "### ⚠️ Risk: Regional Leakage with Random Splits\n",
    "\n",
    "```python\n",
    "# WRONG: Random split across all regions\n",
    "Region A: Train [═══] Test [═══] Train [═══] Test\n",
    "Region B: Test [═══] Train [═══] Test [═══]\n",
    "          \n",
    "Same dates mixed in train and test → LEAKAGE!\n",
    "```\n",
    "\n",
    "**Problem**: If Region A's training data contains the same dates as Region B's test data, the model learns weather patterns that appear in test set.\n",
    "\n",
    "### ✅ Solution: Location-Aware or Time-Based Splitting\n",
    "\n",
    "**Option 1: Time-based split (Current approach)**\n",
    "```python\n",
    "# All regions follow same temporal split\n",
    "split_date = '2023-09-01'\n",
    "train = data[data['date'] < split_date]\n",
    "test = data[data['date'] >= split_date]\n",
    "```\n",
    "\n",
    "**Option 2: Location-based split (Alternative)**\n",
    "```python\n",
    "# Train on some locations, test on others\n",
    "train_locations = regions[:40]  # 80% of locations\n",
    "test_locations = regions[40:]   # 20% of locations\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Feature Importance Example\n",
    "\n",
    "Typical importance ranking for temperature prediction:\n",
    "\n",
    "```\n",
    "1. t2m_mean_last          ███████████████████████ 18.5%\n",
    "2. t2m_mean_recent_3d     ████████████████████ 15.2%\n",
    "3. t2m_mean_mean          ███████████████ 12.8%\n",
    "4. t2m_max_last           ████████████ 10.3%\n",
    "5. t2m_mean_trend         ██████████ 8.7%\n",
    "6. msl_mean_mean          ████████ 6.4%\n",
    "7. t2m_min_last           ███████ 5.9%\n",
    "8. d2m_mean_recent_3d     ██████ 4.8%\n",
    "...\n",
    "```\n",
    "\n",
    "**Key insights**:\n",
    "- Recent values (`_last`, `_recent_3d`) are most important\n",
    "- Mean temperature features dominate\n",
    "- Pressure (`msl`) provides additional signal\n",
    "- Humidity (`d2m`) helps with precision\n",
    "\n",
    "---\n",
    "\n",
    "## 🎓 Summary\n",
    "\n",
    "### What You Have:\n",
    "- ✅ **Window size = 7 days** of historical weather\n",
    "- ✅ **8 statistical features** per original variable\n",
    "- ✅ **Implicit lag structure** (lag 1, lags 1-3, lag 7)\n",
    "- ✅ **Time-based train/test split** (no temporal leakage)\n",
    "\n",
    "### What You DON'T Need:\n",
    "- ❌ Explicit lag features (redundant with current approach)\n",
    "- ❌ More complex lag structures (RF handles this)\n",
    "- ❌ Additional recent_Xd features (recent_3d is sufficient)\n",
    "\n",
    "### When to Add Explicit Lags:\n",
    "Only if:\n",
    "1. Using LSTM/RNN models (need sequential input)\n",
    "2. Model performance is poor (R² < 0.5)\n",
    "3. Domain knowledge suggests specific periodicities\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Best Practice\n",
    "\n",
    "**For Random Forest temperature prediction:**\n",
    "\n",
    "```python\n",
    "# ✅ RECOMMENDED: Current aggregated approach\n",
    "window_size = 7\n",
    "features_per_variable = 8  # mean, std, min, max, last, first, trend, recent_3d\n",
    "\n",
    "# ❌ NOT NEEDED: Explicit lags\n",
    "lag_features = [lag1, lag2, lag3, ...]  # Redundant for RF\n",
    "```\n",
    "\n",
    "**Your current feature engineering is optimal for the task!** 🎯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4776de71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
